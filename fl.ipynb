{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get all the data in ./data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = []\n",
    "def list_files_recursive(path='.'):\n",
    "    for entry in os.listdir(path):\n",
    "        full_path = os.path.join(path, entry)\n",
    "        if os.path.isdir(full_path):\n",
    "            list_files_recursive(full_path)\n",
    "        else:\n",
    "            files.append(full_path)\n",
    "\n",
    "# Specify the directory path you want to start from\n",
    "directory_path = './data'\n",
    "list_files_recursive(directory_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve the data in two different lists (npz files and csv files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFilesNpz = []\n",
    "dataFilesCSV = []\n",
    "for i in files:\n",
    "    if i[-3:] == \"npz\" and (\"reversed\" not in i and \"symetric\" not in i):\n",
    "        dataFilesNpz.append(i)\n",
    "    if i[-3:] == \"csv\":\n",
    "        dataFilesCSV.append(i)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We gather the usefull information for the npz files (Raycast values, speed and current controls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import lzma\n",
    "import re\n",
    "import csv\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "\n",
    "for i in range(len(dataFilesNpz)):\n",
    "    x.append([])\n",
    "    y.append([])\n",
    "\n",
    "for i in dataFilesNpz:\n",
    "    cli = int(re.search(r\"client\\d\", i).group()[-1]) - 1\n",
    "    with lzma.open(i, \"rb\") as file:\n",
    "        data = pickle.load(file)\n",
    "        x[cli].append([[e.raycast_distances, e.car_speed] for e in data])\n",
    "        y[cli].append([e.current_controls for e in data])\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A little processing on the lists to flatten each sub datasets. Previously we had something like \n",
    "\n",
    "[dataset1[dataset1_1, dataset1_2,...], dataset2[dataset2_1, dataset2_2,...],...]\n",
    "\n",
    "and we want [dataset1[all the data from 1], dataset2[all the data from 2],...]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "for i in x:\n",
    "    if len(i) > 0:\n",
    "        i = reduce(lambda a,b:a+b, i)\n",
    "\n",
    "\n",
    "for i in y:\n",
    "    if len(i) > 0:\n",
    "        i = reduce(lambda a,b:a+b, i)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next function is used to separate features from labels and create the tensors. We also split the data int train and test sets\n",
    "\n",
    "We can also specify what protion of the data we want to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessData(data_x, data_y, percent_of_data = 100):\n",
    "\n",
    "    y2 = []\n",
    "    newList = []\n",
    "    for idx, j in enumerate(data_x[0]):\n",
    "        tmp = list(j[0])\n",
    "        tmp.append(j[1])\n",
    "        newList.append(tmp)\n",
    "        newList.append(tmp[::-1])\n",
    "\n",
    "        y2.append(data_y[0][idx])\n",
    "        data_y[0][idx] = list(data_y[0][idx])\n",
    "        tmp2 = data_y[0][idx][2]\n",
    "        data_y[0][idx][2] = data_y[0][idx][3]\n",
    "        data_y[0][idx][3] = tmp2\n",
    "\n",
    "        y2.append(data_y[0][idx])\n",
    "\n",
    "    tensorX = [torch.Tensor(i) for i in newList]\n",
    "    tensorY = [torch.Tensor(i) for i in y2]  \n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(tensorX, \n",
    "                                                    tensorY, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=42) \n",
    "    \n",
    "    train_select = int(len(X_train) * percent_of_data / 100)\n",
    "    test_select = int(len(X_test) * percent_of_data / 100)\n",
    "\n",
    "    X_train = X_train[:train_select]\n",
    "    y_train = y_train[:train_select]\n",
    "    X_test = X_test[:test_select]\n",
    "    y_test = y_test[:test_select]\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class defines the architecture of the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "      super(Net, self).__init__()\n",
    "\n",
    "      self.layIn = nn.Linear(16, 9)\n",
    "\n",
    "      self.fc1 = nn.Linear(9, 9)\n",
    "      self.fc2 = nn.Linear(9, 9)\n",
    "      self.fc3 = nn.Linear(9, 9)\n",
    "\n",
    "      self.out = nn.Linear(9,4)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layIn(x)\n",
    "        x = F.sigmoid(x)\n",
    "        x = self.fc1(x)\n",
    "        x = F.sigmoid(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.sigmoid(x)\n",
    "        x = self.fc3(x)\n",
    "        x = F.sigmoid(x)\n",
    "\n",
    "        out = self.out(x)\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next function is called every epoch. We first infer the test data and compare it to the expected result. We then compute the loss function and do the backward propagation to improve the model. After that we have a bit of code to compute the loss and accuracy for this epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runModel(my_nn, X_train, y_train, X_test, y_test, criterion, optimizer, epoch, verbose = False):\n",
    "    train_loss = 0\n",
    "    my_nn.train()\n",
    "    for idx, i in enumerate(X_train):\n",
    "      # 1. Forward pass (model outputs raw logits)\n",
    "      y_logits = my_nn(i)\n",
    "      y_pred = torch.round(torch.sigmoid(y_logits)) # turn logits -> pred probs -> pred labls\n",
    "\n",
    "      loss = criterion(y_logits, y_train[idx]) \n",
    "      train_loss += loss\n",
    "\n",
    "      # 3. Optimizer zero grad\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      # 4. Loss backwards\n",
    "      loss.backward()\n",
    "\n",
    "      # 5. Optimizer step\n",
    "      optimizer.step()\n",
    "\n",
    "    my_nn.eval()\n",
    "    test_acc = 0\n",
    "    test_loss = 0\n",
    "    train_acc = 0\n",
    "    \n",
    "    if epoch%1 == 0: #was used to test each x epoch\n",
    "      \n",
    "      for idx, i in enumerate(X_test):\n",
    "        \n",
    "        with torch.inference_mode():\n",
    "          # 1. Forward pass\n",
    "          test_logits = my_nn(i).squeeze() \n",
    "          test_pred = torch.round(torch.sigmoid(test_logits))\n",
    "          # 2. Caculate loss/accuracy\n",
    "          test_loss += criterion(test_logits,\n",
    "                              y_test[idx])\n",
    "          for idx, j in enumerate(y_test[idx].tolist()):\n",
    "            if test_pred.tolist()[idx] == j: \n",
    "              test_acc+=1\n",
    "\n",
    "      for idx, i in enumerate(X_train):\n",
    "        \n",
    "        with torch.inference_mode():\n",
    "          # 1. Forward pass\n",
    "          train_logits = my_nn(i).squeeze() \n",
    "          train_pred = torch.round(torch.sigmoid(train_logits))\n",
    "          # 2. Caculate loss/accuracy\n",
    "\n",
    "          for idx, j in enumerate(y_train[idx].tolist()):\n",
    "            if train_pred.tolist()[idx] == j: \n",
    "              train_acc+=1\n",
    "      \n",
    "      if verbose:\n",
    "        print(f\"epoch {epoch}: Acc: {test_acc/(4*len(X_test))}, loss: {test_loss/len(X_test)}\")\n",
    "      \n",
    "      return [test_loss/len(X_test), train_loss/len(X_train)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getData(pos, percent = 100):\n",
    "    X_train, X_test, y_train, y_test = preprocessData(x[pos], y[pos], percent)\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now start the federated learning part. We select a small sample from all the data (10% here) and pretrain a base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline for training\n",
    "# First base model training\n",
    "import copy\n",
    "globalNN = Net()\n",
    "criterion = nn.CrossEntropyLoss(torch.Tensor([1,1,1,1]))\n",
    "optimizer = optim.Adam(globalNN.parameters(), lr=0.001)\n",
    "base_optim = copy.deepcopy(optimizer)\n",
    "\n",
    "#setup 10% of the data\n",
    "X_train_start = []\n",
    "y_train_start = []\n",
    "X_test_start = []\n",
    "y_test_start = []\n",
    "\n",
    "for i in range(6):\n",
    "    X_train, X_test, y_train, y_test = preprocessData(x[i], y[i], 10)\n",
    "\n",
    "    X_train_start.append(X_train)\n",
    "    y_train_start.append(y_train)\n",
    "    X_test_start.append(X_test)\n",
    "    y_test_start.append(y_test)\n",
    "\n",
    "\n",
    "X_train_start = reduce(lambda a,b:a+b, X_train_start)\n",
    "y_train_start = reduce(lambda a,b:a+b, y_train_start)\n",
    "X_test_start = reduce(lambda a,b:a+b, X_test_start)\n",
    "y_test_start = reduce(lambda a,b:a+b, y_test_start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: Acc: 0.5219435736677116, loss: 1.586965560913086\n",
      "epoch 1: Acc: 0.707680250783699, loss: 1.5311529636383057\n",
      "epoch 2: Acc: 0.731974921630094, loss: 1.3761284351348877\n",
      "epoch 3: Acc: 0.7523510971786834, loss: 1.32063889503479\n",
      "epoch 4: Acc: 0.7703761755485894, loss: 1.2734323740005493\n",
      "epoch 5: Acc: 0.780564263322884, loss: 1.258720874786377\n",
      "epoch 6: Acc: 0.7594043887147336, loss: 1.2641924619674683\n",
      "epoch 7: Acc: 0.7735109717868338, loss: 1.2575322389602661\n",
      "epoch 8: Acc: 0.7664576802507836, loss: 1.244807243347168\n",
      "epoch 9: Acc: 0.7852664576802508, loss: 1.2388451099395752\n",
      "epoch 10: Acc: 0.7821316614420063, loss: 1.2474243640899658\n",
      "epoch 11: Acc: 0.7844827586206896, loss: 1.2253687381744385\n",
      "epoch 12: Acc: 0.7617554858934169, loss: 1.219712257385254\n",
      "epoch 13: Acc: 0.7907523510971787, loss: 1.2107442617416382\n",
      "epoch 14: Acc: 0.7899686520376176, loss: 1.2067759037017822\n",
      "epoch 15: Acc: 0.7727272727272727, loss: 1.1966116428375244\n",
      "epoch 16: Acc: 0.7719435736677116, loss: 1.1898484230041504\n",
      "epoch 17: Acc: 0.7633228840125392, loss: 1.217958688735962\n",
      "epoch 18: Acc: 0.7617554858934169, loss: 1.2213619947433472\n",
      "epoch 19: Acc: 0.7648902821316614, loss: 1.223876714706421\n",
      "epoch 20: Acc: 0.7774294670846394, loss: 1.185558795928955\n",
      "epoch 21: Acc: 0.7782131661442007, loss: 1.1878314018249512\n",
      "epoch 22: Acc: 0.774294670846395, loss: 1.1885305643081665\n",
      "epoch 23: Acc: 0.7727272727272727, loss: 1.1769675016403198\n",
      "epoch 24: Acc: 0.7813479623824452, loss: 1.173130989074707\n",
      "epoch 25: Acc: 0.7758620689655172, loss: 1.1719775199890137\n",
      "epoch 26: Acc: 0.7797805642633229, loss: 1.1637529134750366\n",
      "epoch 27: Acc: 0.7727272727272727, loss: 1.1648887395858765\n",
      "epoch 28: Acc: 0.7719435736677116, loss: 1.1632506847381592\n",
      "epoch 29: Acc: 0.774294670846395, loss: 1.1561131477355957\n",
      "epoch 30: Acc: 0.7719435736677116, loss: 1.1746896505355835\n",
      "epoch 31: Acc: 0.7735109717868338, loss: 1.1618036031723022\n",
      "epoch 32: Acc: 0.7899686520376176, loss: 1.1638480424880981\n",
      "epoch 33: Acc: 0.7719435736677116, loss: 1.153679609298706\n",
      "epoch 34: Acc: 0.7719435736677116, loss: 1.1532304286956787\n",
      "epoch 35: Acc: 0.7703761755485894, loss: 1.1576930284500122\n",
      "epoch 36: Acc: 0.7695924764890282, loss: 1.1892852783203125\n",
      "epoch 37: Acc: 0.7688087774294671, loss: 1.1594557762145996\n",
      "epoch 38: Acc: 0.7648902821316614, loss: 1.171121597290039\n",
      "epoch 39: Acc: 0.7633228840125392, loss: 1.1577228307724\n",
      "epoch 40: Acc: 0.7656739811912225, loss: 1.169418454170227\n",
      "epoch 41: Acc: 0.7648902821316614, loss: 1.1631678342819214\n",
      "epoch 42: Acc: 0.7664576802507836, loss: 1.1535791158676147\n",
      "epoch 43: Acc: 0.7641065830721003, loss: 1.1568619012832642\n",
      "epoch 44: Acc: 0.762539184952978, loss: 1.1583665609359741\n",
      "epoch 45: Acc: 0.7656739811912225, loss: 1.1469266414642334\n",
      "epoch 46: Acc: 0.7633228840125392, loss: 1.1438549757003784\n",
      "epoch 47: Acc: 0.7633228840125392, loss: 1.144231915473938\n",
      "epoch 48: Acc: 0.762539184952978, loss: 1.1415294408798218\n",
      "epoch 49: Acc: 0.7594043887147336, loss: 1.1675426959991455\n",
      "epoch 50: Acc: 0.7617554858934169, loss: 1.1377878189086914\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(51): # Pretrained base model\n",
    "    runModel(globalNN, X_train_start, y_train_start, X_test_start, y_test_start, criterion, optimizer, epoch, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we computed the base model, We create workers for each sub datasets. We store them in a list. We have to be careful when lodaing the weights because we will use processes to make them work in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelsList = []\n",
    "for i in range(6):\n",
    "    model_clone = Net()\n",
    "    model_clone.load_state_dict(copy.deepcopy(globalNN.state_dict()))\n",
    "    modelsList.append(model_clone)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next function creates processes and runs the optimisation on each worker. We can see that each process fetches the specific data for its worker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating threads and launching individual data\n",
    "import concurrent\n",
    "from concurrent import futures\n",
    "\n",
    "\n",
    "    \n",
    "def thread(x):\n",
    "    try:\n",
    "        data = getData(x)\n",
    "        opt = optim.Adam(modelsList[x].parameters(), lr=0.001)\n",
    "        for i in range(50):\n",
    "            r = runModel(modelsList[x], data[0], data[1], data[2], data[3], criterion, opt, i)\n",
    "        return modelsList[x]\n",
    "    except Exception as e:\n",
    "        print(f\"oh no: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runThreads():\n",
    "    executor = concurrent.futures.ProcessPoolExecutor(10)\n",
    "    futures = [executor.submit(thread, i) for i in range(6)]\n",
    "    futuresDone = concurrent.futures.wait(futures)[0]\n",
    "    print(\"done\")\n",
    "    return [x.result() for x in futuresDone]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we compute the weights average after each round of optimisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAvgWeights(mod):\n",
    "    weights = list(map(lambda x: copy.deepcopy(x.state_dict()), mod))\n",
    "    avg_weights = copy.deepcopy(weights[0])\n",
    "    for key in avg_weights:\n",
    "        avg_weights[key] = (weights[0][key] + weights[1][key] + weights[2][key] + weights[3][key] + weights[4][key] + weights[5][key]) / 6\n",
    "    return avg_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part is meant to launch the worker threads a number of times and create new wokers with the new weights once a round is completed. We finally get the final weigths for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pass 0 through models\n",
      "done\n",
      "calc new weights\n",
      "pass 1 through models\n",
      "done\n",
      "calc new weights\n",
      "pass 2 through models\n",
      "done\n",
      "calc new weights\n",
      "pass 3 through models\n",
      "done\n",
      "calc new weights\n",
      "pass 4 through models\n",
      "done\n",
      "calc new weights\n",
      "pass 5 through models\n",
      "done\n",
      "calc new weights\n",
      "pass 6 through models\n",
      "done\n",
      "calc new weights\n",
      "pass 7 through models\n",
      "done\n",
      "calc new weights\n",
      "pass 8 through models\n",
      "done\n",
      "calc new weights\n",
      "pass 9 through models\n",
      "done\n",
      "calc new weights\n",
      "pass 10 through models\n",
      "done\n",
      "calc new weights\n",
      "pass 11 through models\n",
      "done\n",
      "calc new weights\n",
      "pass 12 through models\n",
      "done\n",
      "calc new weights\n",
      "pass 13 through models\n",
      "done\n",
      "calc new weights\n",
      "pass 14 through models\n",
      "done\n",
      "calc new weights\n"
     ]
    }
   ],
   "source": [
    "for i in range(15):\n",
    "    print(f\"pass {i} through models\")\n",
    "    modelsList = runThreads()\n",
    "    print(\"calc new weights\")\n",
    "    new_weights = getAvgWeights(modelsList)\n",
    "    modelsList = []\n",
    "    for i in range(6):\n",
    "        model_clone = Net()\n",
    "        model_clone.load_state_dict(copy.deepcopy(new_weights))\n",
    "        modelsList.append(model_clone)\n",
    "\n",
    "finalWeights = getAvgWeights(modelsList)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict({'layIn.weight': tensor([[ 6.7935e-01, -1.7538e-01,  1.1575e+00,  2.6148e-01, -2.6437e-01,\n",
      "          3.0288e-02,  4.2055e-02, -4.2197e-01, -8.8527e-01, -4.2597e-01,\n",
      "         -6.2621e-02, -3.5026e-01, -1.4880e-01,  1.0666e-01,  4.0410e-02,\n",
      "         -3.4778e-01],\n",
      "        [ 2.8451e-01,  2.5293e-01,  4.0484e-01,  1.4559e-01,  2.8419e-01,\n",
      "          2.0638e-01,  5.2034e-01,  7.1690e-01,  4.7914e-01,  1.0249e-01,\n",
      "          2.6836e-01, -2.7233e-01,  3.5176e-01,  1.7120e-01,  5.1377e-01,\n",
      "          4.7481e-01],\n",
      "        [ 4.9147e-01,  4.4576e-02, -1.5631e-01, -1.4679e-01, -1.0776e-01,\n",
      "          9.0382e-02,  3.1270e-01,  2.3470e-01,  6.1215e-01,  4.8834e-01,\n",
      "          2.7798e-01,  2.9586e-01,  1.6758e-01,  3.2600e-01,  2.7632e-01,\n",
      "          5.8326e-01],\n",
      "        [-7.8664e-01, -5.6801e-01, -4.3534e-01, -4.1508e-01, -6.2161e-01,\n",
      "         -4.0843e-01, -5.4060e-01, -6.0136e-01, -3.3182e-01, -1.2958e-01,\n",
      "         -1.8029e-01,  4.1025e-01, -1.2986e-01,  5.5606e-02, -2.8897e-01,\n",
      "         -8.8357e-02],\n",
      "        [-3.7596e-01,  6.2642e-02,  1.6507e-01, -1.4957e-01, -1.1218e-01,\n",
      "         -1.5937e-01, -5.0414e-01, -4.3044e-01, -3.7780e-01,  1.4464e-01,\n",
      "          1.8484e-02,  2.4451e-02,  9.5462e-01,  6.2048e-02, -2.2521e-01,\n",
      "          3.0006e-01],\n",
      "        [ 5.9569e-01,  4.7879e-01, -3.5400e-02, -3.1165e-01, -2.5156e-01,\n",
      "         -2.0527e-01,  1.4438e-01,  5.8798e-01,  6.2355e-01,  5.4509e-01,\n",
      "          4.8787e-01,  4.1533e-01,  4.4887e-01,  3.6190e-01,  4.6361e-01,\n",
      "          2.8460e-01],\n",
      "        [ 3.0551e-02,  3.1684e-01, -1.3280e+00, -1.4607e+00, -4.6078e-01,\n",
      "          3.4136e-01,  8.8413e-01,  1.2835e+00, -1.7110e-01, -1.1019e-01,\n",
      "         -2.1746e-02, -1.8636e-01,  2.8371e-01, -8.2359e-02, -5.9223e-01,\n",
      "         -7.5416e-01],\n",
      "        [-2.7012e-01, -2.3885e-01,  3.5644e-01, -1.4184e-02, -5.2066e-01,\n",
      "          2.8701e-02, -5.8073e-04,  1.5600e-01,  6.8659e-01,  5.2272e-01,\n",
      "          1.4372e-01,  1.5607e-01, -3.3619e-01, -1.2911e+00, -2.9845e-01,\n",
      "          1.6789e-01],\n",
      "        [-6.4501e-01, -4.7372e-01,  2.6340e-01,  8.0388e-02,  7.7255e-01,\n",
      "         -5.6181e-01, -2.2390e-01, -1.5810e-02,  3.5395e-01,  1.2493e+00,\n",
      "          4.3971e-01, -1.5556e+00, -6.8867e-01, -8.9709e-01,  7.9699e-01,\n",
      "          1.1592e-01]]), 'layIn.bias': tensor([ 3.1961, -0.4368,  0.1269,  0.9394,  6.6958,  0.1734, -1.3731, -5.0233,\n",
      "        -3.7361]), 'fc1.weight': tensor([[  2.8092,  -1.4162,   0.6807,   1.0700, -10.9605,   0.1595,   2.3704,\n",
      "          -1.0452,  -2.1994],\n",
      "        [ -8.0598,   0.4748,  -2.0020,   0.7764,   5.6598,  -0.6209,  -2.1410,\n",
      "           1.5830,   1.8188],\n",
      "        [  3.6469,  -0.6662,   1.4833,   2.8099,  -2.8493,   1.1710,  -0.0830,\n",
      "          -3.8856, -15.4047],\n",
      "        [ -5.5750,  -0.4581,   0.3472,  -2.0424,  -1.7723,  -0.7532,   3.4541,\n",
      "           7.2532,  -1.6588],\n",
      "        [  6.5784,  -0.5960,   1.6373,   1.8326,  -5.8438,   0.9475,   0.7213,\n",
      "          -2.8942,  -1.6822],\n",
      "        [ -4.0767,   1.1549,  -1.2498,  -0.8765,   4.4863,  -0.5436,  -7.8967,\n",
      "           0.8048,  -2.4544],\n",
      "        [  7.1212,  -0.3454,   2.1310,   3.0651,  -5.2750,   0.8047,   0.8346,\n",
      "          -3.0804,  -1.8576],\n",
      "        [  3.9381,  -1.6609,   0.6544,   1.7533,  -8.3986,   0.3529,   2.4870,\n",
      "          -0.7920,  -2.4130],\n",
      "        [  6.2676,  -0.7633,   1.5538,   2.2893,  -6.0175,   0.3019,   0.4775,\n",
      "          -3.1974,  -0.9697]]), 'fc1.bias': tensor([-0.7171, -0.1853, -0.2594, -1.3013, -0.1035,  0.4185,  0.1505, -1.2227,\n",
      "        -0.1058]), 'fc2.weight': tensor([[-2.6586,  2.3317,  0.9976,  0.9022, -3.0345,  3.8361, -3.4282, -3.1125,\n",
      "         -2.9017],\n",
      "        [ 2.3302, -1.4228, -1.9211,  0.2851,  1.4637, -4.2798,  1.9265,  2.3238,\n",
      "          2.0246],\n",
      "        [-1.4350,  1.4753,  2.0562,  0.2216, -1.6833,  4.4027, -2.6802, -1.9873,\n",
      "         -1.8367],\n",
      "        [-0.5786,  0.8489, -5.1584,  2.3874, -2.0378, -2.1145, -2.2145, -0.9116,\n",
      "         -2.4162],\n",
      "        [-0.0685, -1.1146, -2.0402,  2.6708, -0.0410, -3.3467, -0.2601,  0.4444,\n",
      "          0.0078],\n",
      "        [ 3.0922, -1.6303, -2.0492, -0.8809,  2.8119, -5.2797,  3.4013,  3.5065,\n",
      "          2.9053],\n",
      "        [-0.5745, -0.1034,  2.7553, -1.0110,  0.2139,  3.8459,  0.1382, -0.8952,\n",
      "          0.7568],\n",
      "        [-0.6978, -1.5757, -0.0603,  3.3650, -0.0999,  0.7823, -0.5956, -1.2888,\n",
      "         -0.2290],\n",
      "        [ 1.8471, -0.7078, -0.1224, -0.7027,  1.4292, -5.2974,  1.5938,  1.5616,\n",
      "          1.1496]]), 'fc2.bias': tensor([ 0.4672, -0.3941,  0.2814,  0.9857,  0.1884, -0.0572,  0.2208,  0.7430,\n",
      "        -0.4530]), 'fc3.weight': tensor([[-0.4039,  1.0473, -1.2282, -1.3532,  0.2114,  1.2220, -3.1808, -1.0608,\n",
      "          0.8505],\n",
      "        [ 3.0917, -1.7964,  2.9452,  2.1853, -0.7186, -2.3357,  1.2270,  1.3908,\n",
      "         -1.8698],\n",
      "        [ 1.5291, -0.9346,  0.9242,  2.7792,  0.4537, -2.5750, -2.6337,  1.1549,\n",
      "         -0.8950],\n",
      "        [-1.9906, -1.3228, -2.5110, -0.3641, -0.4962, -2.9396, -4.8456, -1.5326,\n",
      "         -1.8215],\n",
      "        [-2.1313,  0.7810, -3.0325, -5.1352, -0.2180,  0.3981, -0.0944, -3.0587,\n",
      "          0.5666],\n",
      "        [-0.4319,  5.0189, -0.7630,  1.0590,  4.7420,  1.0574, -2.2882,  0.4697,\n",
      "          3.4686],\n",
      "        [-2.3870,  1.0569, -2.2925,  0.5436,  2.2926,  0.0352, -4.1891, -0.2418,\n",
      "         -0.6811],\n",
      "        [ 0.6890, -2.2995,  1.0157,  0.8110, -1.0918, -3.6301,  0.3380,  0.0978,\n",
      "         -2.8833],\n",
      "        [-3.9041,  2.9617, -2.1037, -1.7861,  1.1455,  4.3497, -0.7514, -0.1935,\n",
      "          2.4647]]), 'fc3.bias': tensor([-0.4537, -0.6315, -0.8016, -2.8970, -0.2791,  0.3543, -1.4783, -1.2564,\n",
      "        -0.1211]), 'out.weight': tensor([[ -0.4054,  -0.3806,   0.2995,  -0.5571,  -5.2091,   0.2225,   0.0350,\n",
      "          -2.4853,   0.4669],\n",
      "        [  0.3033,  -1.0726,  -1.7952,  -0.2472,   2.0446,  -1.5894,  -4.3573,\n",
      "           0.2681,  -1.6216],\n",
      "        [ -4.1946,  -0.0893,  -0.7743,  -1.0581, -14.2831,  -0.4709,  -0.4217,\n",
      "          -0.4530,  -0.6461],\n",
      "        [ -1.3794,  -1.8770,  -1.3690,  -1.2277,  -0.4939,   0.3827,  -0.3792,\n",
      "          -8.1535,  -0.3606]]), 'out.bias': tensor([-0.9842,  0.0915,  0.1212,  0.0505])})\n"
     ]
    }
   ],
   "source": [
    "print(finalWeights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the end, we save the weights to the files to be used in the autopilote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.save(finalWeights, \"best_FL_model.pt\")\n",
    "finalNN = Net()\n",
    "finalNN.load_state_dict(finalWeights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39msigmoid(finalNN(X_train[\u001b[38;5;241m15\u001b[39m]))\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#torch.sigmoid(globalNN(X_train[15]))\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "torch.sigmoid(finalNN(X_train[15]))\n",
    "#torch.sigmoid(globalNN(X_train[15]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
